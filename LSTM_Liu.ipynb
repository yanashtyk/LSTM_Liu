{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile, flare_label, series_len, start_feature, n_features, mask_value):\n",
    "    df=pd.read_csv(datafile)\n",
    "    df_values=df.values\n",
    "    X=[]\n",
    "    y=[]\n",
    "    tmp=[]\n",
    "    for k in range (start_feature, start_feature+n_features):\n",
    "        tmp.append(mask_value)\n",
    "    for idx in range(0, len(df_values)):\n",
    "        each_series_data=[]\n",
    "        row=df_values[idx]\n",
    "        if flare_label == 'M5' and row[1][0]=='M' and float(row[1][1:]) >=5.0:\n",
    "            label='X'\n",
    "        else:\n",
    "            label=row[1][0]\n",
    "        if flare_label=='M' and label=='X':\n",
    "            label='M'\n",
    "        if flare_label=='C' and (label=='X' or label=='M'):\n",
    "            label='C'\n",
    "        if flare_label=='B' and (label=='X' or label=='M' or label=='C'):\n",
    "            label='B'\n",
    "        if flare_label=='M5' and (label=='M' or label=='C' or label=='B'):\n",
    "            label='N'\n",
    "        if flare_label=='M' and (label=='B' or label=='C'):\n",
    "            label='N'\n",
    "        if flare_label=='C' and label=='B':\n",
    "            label='N'\n",
    "        has_zero_record=False\n",
    "        #if at least one of the 25 physical feature values is missing, then discard it\n",
    "        if flare_label=='C':\n",
    "            if float(row[5])==0.0:\n",
    "                has_zero_record=True\n",
    "            if float(row[7])==0.0:\n",
    "                has_zero_record=True\n",
    "            for k in range(9, 13):\n",
    "                if float(row[k])==0.0:\n",
    "                    has_zero_record=True\n",
    "                    break\n",
    "            for k in range(14, 16):\n",
    "                if float(row[k])==0.0:\n",
    "                    has_zero_record=True\n",
    "                    break\n",
    "            for k in range(18, 21):\n",
    "                if float(row[k])==0.0:\n",
    "                    has_zero_record=True\n",
    "                    break\n",
    "            if float(row[22])==0.0:\n",
    "                has_zero_record=True\n",
    "            for k in range(24, 33):\n",
    "                if float(row[k])==0.0:\n",
    "                    has_zero_record=True\n",
    "                    break\n",
    "            for k in range(38, 42):\n",
    "                if float(row[k])==0.0:\n",
    "                    has_zero_record=True\n",
    "                    break\n",
    "            #check only for C flares prediction\n",
    "            \n",
    "            if has_zero_record is False:\n",
    "                cur_noaa_num=int(row[3])\n",
    "                each_series_data.append(row[start_feature:start_feature+n_features].tolist())\n",
    "                itr_idx=idx-1\n",
    "                while itr_idx>=0 and len(each_series_data)<series_len:\n",
    "                    prev_row=df_values[itr_idx]\n",
    "                    prev_noaa_num=int(prev_row[3])\n",
    "                    if prev_noaa_num!=cur_noaa_num:\n",
    "                        break\n",
    "                    has_zero_record_tmp=False\n",
    "                    if flare_label=='C':\n",
    "                        if float(row[5])==0.0:\n",
    "                            has_zero_record_tmp=True\n",
    "                        if float(row[7])==0.0:\n",
    "                            has_zero_record_tmp=True\n",
    "                        for k in range(9, 13):\n",
    "                            if float(row[k])==0.0:\n",
    "                                has_zero_record_tmp=True\n",
    "                                break\n",
    "                        for k in range(14, 16):\n",
    "                            if float(row[k])==0.0:\n",
    "                                has_zero_record_tmp=True\n",
    "                                break\n",
    "                        for k in range(18, 21):\n",
    "                            if float(row[k])==0.0:\n",
    "                                has_zero_record_tmp=True\n",
    "                                break\n",
    "                        if float(row[22])==0.0:\n",
    "                                has_zero_record_tmp=True\n",
    "                        for k in range(24, 33):\n",
    "                            if float(row[k])==0.0:\n",
    "                                has_zero_record_tmp=True\n",
    "                                break\n",
    "                        for k in range(38, 42):\n",
    "                            if float(row[k])==0.0:\n",
    "                                has_zero_record_tmp=True\n",
    "                                break\n",
    "                                \n",
    "                                #tested only for C flares\n",
    "                    if len(each_series_data)<series_len and has_zero_record_tmp is True:\n",
    "                        each_series_data.insert(0, tmp)\n",
    "                    \n",
    "                    if len(each_series_data)<series_len and has_zero_record_tmp is False:\n",
    "                        each_series_data.insert(0, prev_row[start_feature:start_feature+n_features].tolist())\n",
    "                    itr_idx-=1\n",
    "                while len(each_series_data)>0 and len(each_series_data)<series_len:\n",
    "                    each_series_data.insert(0, tmp)\n",
    "                \n",
    "                if len(each_series_data)>0:\n",
    "                    X.append(np.array(each_series_data).reshape(series_len, n_features).tolist())\n",
    "                    y.append(label)\n",
    "    X_arr=np.array(X)\n",
    "    y_arr=np.array(y)\n",
    "    print(X_arr.shape)\n",
    "    return X_arr, y_arr\n",
    "\n",
    "\n",
    "                        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84577, 10, 14)\n",
      "(26473, 10, 14)\n",
      "(44689, 10, 14)\n"
     ]
    }
   ],
   "source": [
    "flare_label='C'\n",
    "filepath='./'+flare_label+'/'\n",
    "n_features=14\n",
    "num_of_fold=10\n",
    "start_feature=5\n",
    "mask_value=0\n",
    "series_len=10\n",
    "epochs=7\n",
    "batch_size=256\n",
    "nclass=2\n",
    "thlistsize=201\n",
    "thlist=np.linspace(0, 1, thlistsize)\n",
    "\n",
    "X_train_data, y_train_data = load_data(datafile=filepath+'normalized_training.csv', \n",
    "                                       flare_label=flare_label,\n",
    "                                       series_len=series_len,\n",
    "                                       start_feature=start_feature,\n",
    "                                       n_features=n_features,\n",
    "                                       mask_value=mask_value)\n",
    "\n",
    "X_valid_data, y_valid_data=load_data(datafile=filepath+'normalized_validation.csv', \n",
    "                                     flare_label=flare_label,\n",
    "                                     series_len=series_len,\n",
    "                                     start_feature=start_feature,\n",
    "                                     n_features=n_features,\n",
    "                                     mask_value=mask_value)\n",
    "\n",
    "X_test_data, y_test_data=load_data(datafile=filepath+'normalized_testing.csv',\n",
    "                                   flare_label=flare_label,\n",
    "                                   series_len=series_len,\n",
    "                                   start_feature=start_feature,\n",
    "                                   n_features=n_features,\n",
    "                                   mask_value=mask_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_10_folds(X, y, num_of_fold):\n",
    "    num=len(X)\n",
    "    index=[i for i in range(num)]\n",
    "    np.random.seed(123)\n",
    "    np.random.shuffle(index)\n",
    "    X_output=[]\n",
    "    y_output=[]\n",
    "    num_in_each_fold=round(num/num_of_fold)\n",
    "    for i in range(num_of_fold):\n",
    "        if i == (num_of_fold-1):\n",
    "            idx=index[num_in_each_fold*(num_of_fold-1):]\n",
    "        else:\n",
    "            idx=index[num_in_each_fold*i:num_in_each_fold*(i+1)]\n",
    "        X_output.append(X[idx])\n",
    "        y_output.append(y[idx])\n",
    "    return X_output, y_output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(data):\n",
    "    encoder=LabelEncoder()\n",
    "    encoder.fit(data)\n",
    "    encoded_Y=encoder.transform(data)\n",
    "    print(encoded_Y)\n",
    "    converteddata=F.one_hot(torch.Tensor(encoded_Y).long())\n",
    "    return converteddata.float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform_ce(data):\n",
    "    encoder=LabelEncoder()\n",
    "    encoder.fit(data)\n",
    "    encoded_Y=encoder.transform(data)\n",
    "    return encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_layer, vocab_size, **kwards):\n",
    "        super(RNNModel, self).__init__(**kwards)\n",
    "        self.rnn=rnn_layer\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_hiddens=self.rnn.hidden_size\n",
    "        #If the RNN is bedirectional, \n",
    "        #'num_directions' should be 2 , else it shuld be 1.\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions=1\n",
    "            self.drop=nn.Dropout(0.5)\n",
    "            self.linear=nn.Linear(self.num_hiddens, self.num_hiddens)\n",
    "            self.linear1=nn.Linear(self.num_hiddens*2, self.num_hiddens, bias=False)\n",
    "            self.linear2=nn.Linear(self.num_hiddens, 200)\n",
    "            self.linear3=nn.Linear(200, 500)\n",
    "            self.linear4=nn.Linear(500, 2)\n",
    "        else:\n",
    "            self.num_directions=2\n",
    "            self.linear=nn.Linear(self.num_hiddens*2, self.vocab_size)\n",
    "            \n",
    "    def forward(self, inputs, state):\n",
    "        #X=F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        #X=X.to(torch.float32)\n",
    "        inputs=inputs.permute(1, 0, 2)\n",
    "        Y, state=self.rnn(inputs, state)\n",
    "        #The fully connected layer will first change the shape of 'Y' to\n",
    "        # ('num_steps'*'batch_size', 'num_hiddens'). Its output shape is\n",
    "        #('num_steps'*'batch_size', 'vocab_size').\n",
    "        h_t=Y[-1, :, :]\n",
    "        Y=self.drop(Y)\n",
    "        output=self.linear(Y)\n",
    "        o=output.permute(1, 2, 0)\n",
    "        h=h_t.unsqueeze(1)\n",
    "        score=torch.bmm(h, o)\n",
    "        s=score.squeeze(1)\n",
    "        activation_weight=F.softmax(s, dim=1)\n",
    "        o1=output.permute(1, 2, 0)\n",
    "        s1=activation_weight.unsqueeze(2)\n",
    "        context_vector=torch.bmm(o1, s1)\n",
    "        context_vector=context_vector.squeeze(2)\n",
    "        pre_activation=torch.cat((context_vector, h_t), dim=1)\n",
    "        attention_vector=self.linear1(pre_activation)\n",
    "        attention_vector=torch.tanh(attention_vector)\n",
    "        layer1_out=self.linear2(attention_vector)\n",
    "        layer1_out=F.relu(layer1_out)\n",
    "        layer2_out=self.linear3(layer1_out)\n",
    "        layer2_out=F.relu(layer2_out)\n",
    "        model_output=self.linear4(layer2_out)\n",
    "        model_output=F.softmax(model_output, dim=1)\n",
    "        return state, model_output\n",
    "    \n",
    "    def begin_state(self, batch_size=1):\n",
    "            #'nn.LSTM takes a tuple of hidden states'\n",
    "            return (torch.zeros((self.num_directions*self.rnn.num_layers, batch_size, self.num_hiddens)), \n",
    "                    torch.zeros((self.num_directions*self.rnn.num_layers, batch_size, self.num_hiddens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(model, train_iter, train_exp,  updater, loss,  #@save\n",
    "                    use_random_iter):\n",
    "    \"\"\"Train a model within one epoch (defined in Chapter 8).\"\"\"\n",
    "    state= None\n",
    "    #metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    count=0\n",
    "    for X, Y in train_iter:\n",
    "        if count!=train_exp:\n",
    "            \n",
    "            if state is None or use_random_iter:\n",
    "                # Initialize `state` when either it is the first iteration or\n",
    "                # using random sampling\n",
    "                state = model.begin_state(batch_size=8457)\n",
    "            else:\n",
    "                if isinstance(model, nn.Module) and not isinstance(state, tuple):\n",
    "                    # `state` is a tensor for `nn.GRU`\n",
    "                    state.detach_()\n",
    "                else:\n",
    "                    # `state` is a tuple of tensors for `nn.LSTM` and\n",
    "                    # for our custom scratch implementation \n",
    "                    for s in state:\n",
    "                        s.detach_()\n",
    "            #y = Y.T.reshape(-1)\n",
    "           # X, y = X.to(device), y.to(device)\n",
    "           \n",
    "            state, output = model(X, state)\n",
    "           \n",
    "            l=loss(output, Y.long() )+torch.tensor(1.0/8457)*torch.tensor(0.0001)*torch.sum(output**2)\n",
    "            #l=torch.tensor(1/845)*(-torch.sum(class_weights*y*torch.log(output)+output**2))\n",
    "            #l = loss(y_hat, y.long()).mean()\n",
    "            if isinstance(updater, torch.optim.Optimizer):\n",
    "                updater.zero_grad()\n",
    "                l.backward()\n",
    "                #grad_clipping(model, 1)\n",
    "                updater.step()\n",
    "        count+=1\n",
    "            #metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(model, train_iter, train_exp, class_weights, lr, num_epochs, \n",
    "              use_random_iter=False):\n",
    "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
    "\n",
    "    loss=nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Initialize\n",
    "    optim=torch.optim.Adam(model.parameters(), lr , betas = (0.9, 0.999))\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        l = train_epoch_ch8(\n",
    "            model, train_iter,train_exp,  optim,loss, use_random_iter)\n",
    "        print(f'loss {l:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "tensor_x = torch.Tensor(X_train_data)\n",
    "tensor_y = torch.Tensor(data_transform_ce(y_train_data))\n",
    "train_dataset=TensorDataset(tensor_x, tensor_y)\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=8457, shuffle=True, drop_last=True)\n",
    "m=10\n",
    "num_epochs=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_all={0:[], 1:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_dataloader:\n",
    "    class_weights=class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(Y) , y=np.array(Y))\n",
    "    class_weights_all[0].append(class_weights[0])\n",
    "    class_weights_all[1].append(class_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_ts=torch.Tensor(X_test_data)\n",
    "tensor_y_ts=torch.Tensor(data_transform_ce(y_test_data))\n",
    "test_dataset=TensorDataset(tensor_x_ts, tensor_y_ts)\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=4468,  shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "thlistsize = 201\n",
    "thlist = np.linspace(0, 1, thlistsize)\n",
    "\n",
    "max_recall0=np.zeros(thlistsize)\n",
    "max_precision0=np.zeros(thlistsize)\n",
    "max_recall1=np.zeros(thlistsize)\n",
    "max_precision1=np.zeros(thlistsize)\n",
    "max_acc=np.zeros(thlistsize)\n",
    "max_tss=np.zeros(thlistsize)\n",
    "max_bacc=np.zeros(thlistsize)\n",
    "max_hss=np.zeros(thlistsize)\n",
    "recall0list=[]\n",
    "recall1list=[]\n",
    "precision0list=[]\n",
    "precision1list=[]\n",
    "acclist=[]\n",
    "hsslist=[]\n",
    "tsslist=[]\n",
    "bacclist=[]\n",
    "for ithlistsize in range(thlistsize):\n",
    "    recall0list.append([])\n",
    "    recall1list.append([])\n",
    "    precision0list.append([])\n",
    "    precision1list.append([])\n",
    "    acclist.append([])\n",
    "    hsslist.append([])\n",
    "    tsslist.append([])\n",
    "    bacclist.append([])\n",
    "fraction_of_positives_list=[]\n",
    "mean_predicted_value_list=[]\n",
    "fpr_list=[]\n",
    "tpr_list=[]\n",
    "num_of_folds=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 0 iteration-------------\n",
      "loss 0.6487\n",
      "loss 0.5779\n",
      "loss 0.5767\n",
      "loss 0.5759\n",
      "loss 0.5598\n",
      "loss 0.5695\n",
      "loss 0.5640\n",
      "loss 0.5630\n",
      "loss 0.5566\n",
      "loss 0.5498\n",
      "loss 0.5527\n",
      "loss 0.5469\n",
      "loss 0.5453\n",
      "loss 0.5532\n",
      "loss 0.5421\n",
      "loss 0.5465\n",
      "loss 0.5385\n",
      "loss 0.5306\n",
      "loss 0.5386\n",
      "loss 0.5377\n",
      "loss 0.5408\n",
      "loss 0.5237\n",
      "loss 0.5262\n",
      "loss 0.5302\n",
      "loss 0.5243\n",
      "loss 0.5344\n",
      "loss 0.5334\n",
      "loss 0.5321\n",
      "loss 0.5302\n",
      "loss 0.5258\n",
      "loss 0.5274\n",
      "loss 0.5296\n",
      "loss 0.5236\n",
      "loss 0.5329\n",
      "loss 0.5163\n",
      "loss 0.5142\n",
      "loss 0.5255\n",
      "loss 0.5266\n",
      "loss 0.5269\n",
      "loss 0.5184\n",
      "test loss: tensor(0.5083, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5829  1986]\n",
      " [ 5006 27391]]\n",
      "--------- 1 iteration-------------\n",
      "loss 0.6614\n",
      "loss 0.5969\n",
      "loss 0.5828\n",
      "loss 0.5635\n",
      "loss 0.5717\n",
      "loss 0.5657\n",
      "loss 0.5599\n",
      "loss 0.5621\n",
      "loss 0.5621\n",
      "loss 0.5634\n",
      "loss 0.5584\n",
      "loss 0.5513\n",
      "loss 0.5542\n",
      "loss 0.5575\n",
      "loss 0.5596\n",
      "loss 0.5470\n",
      "loss 0.5444\n",
      "loss 0.5327\n",
      "loss 0.5359\n",
      "loss 0.5347\n",
      "loss 0.5375\n",
      "loss 0.5351\n",
      "loss 0.5369\n",
      "loss 0.5321\n",
      "loss 0.5334\n",
      "loss 0.5350\n",
      "loss 0.5214\n",
      "loss 0.5307\n",
      "loss 0.5263\n",
      "loss 0.5282\n",
      "loss 0.5333\n",
      "loss 0.5260\n",
      "loss 0.5273\n",
      "loss 0.5364\n",
      "loss 0.5284\n",
      "loss 0.5222\n",
      "loss 0.5258\n",
      "loss 0.5286\n",
      "loss 0.5251\n",
      "loss 0.5226\n",
      "test loss: tensor(0.5216, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5775  2111]\n",
      " [ 4681 27645]]\n",
      "--------- 2 iteration-------------\n",
      "loss 0.6594\n",
      "loss 0.5916\n",
      "loss 0.5751\n",
      "loss 0.5754\n",
      "loss 0.5712\n",
      "loss 0.5758\n",
      "loss 0.5619\n",
      "loss 0.5615\n",
      "loss 0.5603\n",
      "loss 0.5569\n",
      "loss 0.5602\n",
      "loss 0.5478\n",
      "loss 0.5477\n",
      "loss 0.5509\n",
      "loss 0.5441\n",
      "loss 0.5408\n",
      "loss 0.5406\n",
      "loss 0.5379\n",
      "loss 0.5404\n",
      "loss 0.5363\n",
      "loss 0.5383\n",
      "loss 0.5328\n",
      "loss 0.5329\n",
      "loss 0.5290\n",
      "loss 0.5285\n",
      "loss 0.5305\n",
      "loss 0.5184\n",
      "loss 0.5357\n",
      "loss 0.5233\n",
      "loss 0.5292\n",
      "loss 0.5197\n",
      "loss 0.5183\n",
      "loss 0.5259\n",
      "loss 0.5254\n",
      "loss 0.5289\n",
      "loss 0.5275\n",
      "loss 0.5268\n",
      "loss 0.5245\n",
      "loss 0.5261\n",
      "loss 0.5245\n",
      "test loss: tensor(0.5129, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5614  2189]\n",
      " [ 4688 27721]]\n",
      "--------- 3 iteration-------------\n",
      "loss 0.6549\n",
      "loss 0.5969\n",
      "loss 0.5778\n",
      "loss 0.5797\n",
      "loss 0.5786\n",
      "loss 0.5662\n",
      "loss 0.5632\n",
      "loss 0.5551\n",
      "loss 0.5628\n",
      "loss 0.5447\n",
      "loss 0.5505\n",
      "loss 0.5451\n",
      "loss 0.5401\n",
      "loss 0.5466\n",
      "loss 0.5397\n",
      "loss 0.5389\n",
      "loss 0.5314\n",
      "loss 0.5290\n",
      "loss 0.5373\n",
      "loss 0.5342\n",
      "loss 0.5357\n",
      "loss 0.5312\n",
      "loss 0.5382\n",
      "loss 0.5354\n",
      "loss 0.5301\n",
      "loss 0.5326\n",
      "loss 0.5291\n",
      "loss 0.5264\n",
      "loss 0.5187\n",
      "loss 0.5275\n",
      "loss 0.5365\n",
      "loss 0.5312\n",
      "loss 0.5284\n",
      "loss 0.5271\n",
      "loss 0.5363\n",
      "loss 0.5222\n",
      "loss 0.5183\n",
      "loss 0.5335\n",
      "loss 0.5192\n",
      "loss 0.5226\n",
      "test loss: tensor(0.5090, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5713  2147]\n",
      " [ 4635 27717]]\n",
      "--------- 4 iteration-------------\n",
      "loss 0.6625\n",
      "loss 0.6021\n",
      "loss 0.5780\n",
      "loss 0.5776\n",
      "loss 0.5757\n",
      "loss 0.5663\n",
      "loss 0.5630\n",
      "loss 0.5589\n",
      "loss 0.5484\n",
      "loss 0.5446\n",
      "loss 0.5432\n",
      "loss 0.5368\n",
      "loss 0.5433\n",
      "loss 0.5357\n",
      "loss 0.5399\n",
      "loss 0.5404\n",
      "loss 0.5386\n",
      "loss 0.5328\n",
      "loss 0.5322\n",
      "loss 0.5290\n",
      "loss 0.5377\n",
      "loss 0.5369\n",
      "loss 0.5282\n",
      "loss 0.5227\n",
      "loss 0.5351\n",
      "loss 0.5262\n",
      "loss 0.5307\n",
      "loss 0.5284\n",
      "loss 0.5218\n",
      "loss 0.5268\n",
      "loss 0.5263\n",
      "loss 0.5265\n",
      "loss 0.5261\n",
      "loss 0.5231\n",
      "loss 0.5223\n",
      "loss 0.5290\n",
      "loss 0.5243\n",
      "loss 0.5262\n",
      "loss 0.5307\n",
      "loss 0.5282\n",
      "test loss: tensor(0.5051, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5840  2010]\n",
      " [ 4728 27634]]\n",
      "--------- 5 iteration-------------\n",
      "loss 0.6593\n",
      "loss 0.6129\n",
      "loss 0.5946\n",
      "loss 0.5802\n",
      "loss 0.5802\n",
      "loss 0.5760\n",
      "loss 0.5741\n",
      "loss 0.5633\n",
      "loss 0.5613\n",
      "loss 0.5607\n",
      "loss 0.5578\n",
      "loss 0.5559\n",
      "loss 0.5580\n",
      "loss 0.5517\n",
      "loss 0.5458\n",
      "loss 0.5515\n",
      "loss 0.5372\n",
      "loss 0.5382\n",
      "loss 0.5378\n",
      "loss 0.5279\n",
      "loss 0.5363\n",
      "loss 0.5377\n",
      "loss 0.5326\n",
      "loss 0.5272\n",
      "loss 0.5249\n",
      "loss 0.5262\n",
      "loss 0.5255\n",
      "loss 0.5303\n",
      "loss 0.5246\n",
      "loss 0.5185\n",
      "loss 0.5247\n",
      "loss 0.5335\n",
      "loss 0.5360\n",
      "loss 0.5295\n",
      "loss 0.5286\n",
      "loss 0.5265\n",
      "loss 0.5273\n",
      "loss 0.5206\n",
      "loss 0.5241\n",
      "loss 0.5228\n",
      "test loss: tensor(0.5054, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5684  2166]\n",
      " [ 4652 27710]]\n",
      "--------- 6 iteration-------------\n",
      "loss 0.6455\n",
      "loss 0.5729\n",
      "loss 0.5740\n",
      "loss 0.5788\n",
      "loss 0.5673\n",
      "loss 0.5591\n",
      "loss 0.5599\n",
      "loss 0.5530\n",
      "loss 0.5531\n",
      "loss 0.5463\n",
      "loss 0.5442\n",
      "loss 0.5358\n",
      "loss 0.5390\n",
      "loss 0.5422\n",
      "loss 0.5370\n",
      "loss 0.5384\n",
      "loss 0.5384\n",
      "loss 0.5406\n",
      "loss 0.5359\n",
      "loss 0.5392\n",
      "loss 0.5268\n",
      "loss 0.5262\n",
      "loss 0.5291\n",
      "loss 0.5309\n",
      "loss 0.5290\n",
      "loss 0.5224\n",
      "loss 0.5314\n",
      "loss 0.5240\n",
      "loss 0.5308\n",
      "loss 0.5275\n",
      "loss 0.5206\n",
      "loss 0.5285\n",
      "loss 0.5262\n",
      "loss 0.5252\n",
      "loss 0.5256\n",
      "loss 0.5248\n",
      "loss 0.5281\n",
      "loss 0.5245\n",
      "loss 0.5271\n",
      "loss 0.5294\n",
      "test loss: tensor(0.4982, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5924  1951]\n",
      " [ 5028 27309]]\n",
      "--------- 7 iteration-------------\n",
      "loss 0.6472\n",
      "loss 0.5999\n",
      "loss 0.5969\n",
      "loss 0.5799\n",
      "loss 0.5742\n",
      "loss 0.5733\n",
      "loss 0.5661\n",
      "loss 0.5678\n",
      "loss 0.5554\n",
      "loss 0.5644\n",
      "loss 0.5514\n",
      "loss 0.5496\n",
      "loss 0.5449\n",
      "loss 0.5351\n",
      "loss 0.5439\n",
      "loss 0.5428\n",
      "loss 0.5388\n",
      "loss 0.5350\n",
      "loss 0.5305\n",
      "loss 0.5255\n",
      "loss 0.5290\n",
      "loss 0.5349\n",
      "loss 0.5346\n",
      "loss 0.5310\n",
      "loss 0.5279\n",
      "loss 0.5225\n",
      "loss 0.5330\n",
      "loss 0.5271\n",
      "loss 0.5266\n",
      "loss 0.5355\n",
      "loss 0.5307\n",
      "loss 0.5233\n",
      "loss 0.5256\n",
      "loss 0.5359\n",
      "loss 0.5308\n",
      "loss 0.5246\n",
      "loss 0.5300\n",
      "loss 0.5343\n",
      "loss 0.5193\n",
      "loss 0.5281\n",
      "test loss: tensor(0.4963, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5970  1932]\n",
      " [ 4976 27334]]\n",
      "--------- 8 iteration-------------\n",
      "loss 0.6477\n",
      "loss 0.5919\n",
      "loss 0.5676\n",
      "loss 0.5711\n",
      "loss 0.5657\n",
      "loss 0.5611\n",
      "loss 0.5558\n",
      "loss 0.5474\n",
      "loss 0.5498\n",
      "loss 0.5464\n",
      "loss 0.5425\n",
      "loss 0.5478\n",
      "loss 0.5356\n",
      "loss 0.5358\n",
      "loss 0.5331\n",
      "loss 0.5357\n",
      "loss 0.5422\n",
      "loss 0.5372\n",
      "loss 0.5383\n",
      "loss 0.5296\n",
      "loss 0.5299\n",
      "loss 0.5337\n",
      "loss 0.5319\n",
      "loss 0.5320\n",
      "loss 0.5379\n",
      "loss 0.5298\n",
      "loss 0.5279\n",
      "loss 0.5212\n",
      "loss 0.5253\n",
      "loss 0.5275\n",
      "loss 0.5240\n",
      "loss 0.5256\n",
      "loss 0.5201\n",
      "loss 0.5272\n",
      "loss 0.5309\n",
      "loss 0.5326\n",
      "loss 0.5265\n",
      "loss 0.5252\n",
      "loss 0.5294\n",
      "loss 0.5292\n",
      "test loss: tensor(0.4986, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 5834  1979]\n",
      " [ 4976 27423]]\n",
      "--------- 9 iteration-------------\n",
      "loss 0.6581\n",
      "loss 0.5970\n",
      "loss 0.5859\n",
      "loss 0.5743\n",
      "loss 0.5638\n",
      "loss 0.5740\n",
      "loss 0.5691\n",
      "loss 0.5572\n",
      "loss 0.5501\n",
      "loss 0.5465\n",
      "loss 0.5413\n",
      "loss 0.5432\n",
      "loss 0.5376\n",
      "loss 0.5389\n",
      "loss 0.5399\n",
      "loss 0.5451\n",
      "loss 0.5331\n",
      "loss 0.5364\n",
      "loss 0.5252\n",
      "loss 0.5343\n",
      "loss 0.5386\n",
      "loss 0.5373\n",
      "loss 0.5298\n",
      "loss 0.5328\n",
      "loss 0.5239\n",
      "loss 0.5296\n",
      "loss 0.5308\n",
      "loss 0.5259\n",
      "loss 0.5330\n",
      "loss 0.5299\n",
      "loss 0.5236\n",
      "loss 0.5253\n",
      "loss 0.5276\n",
      "loss 0.5271\n",
      "loss 0.5273\n",
      "loss 0.5320\n",
      "loss 0.5193\n",
      "loss 0.5319\n",
      "loss 0.5189\n",
      "loss 0.5246\n",
      "test loss: tensor(0.5112, grad_fn=<AddBackward0>)\n",
      "\n",
      "0.5\n",
      "[[ 6069  1810]\n",
      " [ 5867 26466]]\n"
     ]
    }
   ],
   "source": [
    "for train_itr in range(num_of_folds):\n",
    "        print('--------- '+str(train_itr) + ' iteration-------------')\n",
    "        \n",
    "        test_itr=train_itr\n",
    "        classweights0=(np.sum(class_weights_all[0])-class_weights_all[0][train_itr])/num_of_folds\n",
    "        classweights1=(np.sum(class_weights_all[1])-class_weights_all[1][train_itr])/num_of_folds\n",
    "        classweights=np.array([classweights0, classweights1])\n",
    "        classweights=torch.Tensor(classweights)\n",
    "        \n",
    "        lstm_layer=nn.LSTM(n_features, m)\n",
    "        model=RNNModel(lstm_layer, m)\n",
    "        \n",
    "        train_ch8(model, train_dataloader, train_itr, classweights, lr, num_epochs)\n",
    "        \n",
    "        y_ts_all=torch.tensor([])\n",
    "        output_ts_all=torch.tensor([])\n",
    "        \n",
    "        count=0\n",
    "        for i in test_dataloader:\n",
    "            if count!=test_itr:\n",
    "                x, y=i\n",
    "                output=model(x, model.begin_state(x.shape[0]))\n",
    "                y_ts_all=torch.cat((y, y_ts_all))\n",
    "                output_ts_all=torch.cat((output[1],output_ts_all), dim=0)\n",
    "            count+=1\n",
    "        loss=nn.CrossEntropyLoss(classweights)\n",
    "        l=loss(output[1], y.long() )+torch.tensor(1.0/x.shape[0])*torch.tensor(0.0001)*torch.sum(output[1]**2)\n",
    "        print('test loss: ' + str(l)+ '\\n')\n",
    "        \n",
    "        \n",
    "        for th in range(len(thlist)):\n",
    "            idx = th\n",
    "            thresh=thlist[idx]\n",
    "            cmat=confusion_matrix(y_ts_all, np.array([output_ts_all[i][0]<(thresh) for i in range (output_ts_all.shape[0])]))\n",
    "            N=np.sum(cmat)\n",
    "            if th==100:\n",
    "                print (thresh)\n",
    "                print(cmat)\n",
    "            if train_itr==4:\n",
    "                tpr=float(cmat[1][1])/float(cmat[1][1]+cmat[1][0])\n",
    "                fpr=float(cmat[0][1])/float(cmat[0][1]+cmat[0][0])\n",
    "                tpr_list.append(tpr)\n",
    "                fpr_list.append(fpr)\n",
    "                \n",
    "            recall=np.zeros(nclass)\n",
    "            precision=np.zeros(nclass)\n",
    "            accuracy=np.zeros(nclass)\n",
    "            bacc=np.zeros(nclass)\n",
    "            tss=np.zeros(nclass)\n",
    "            hss=np.zeros(nclass)\n",
    "            tp=np.zeros(nclass)\n",
    "            fn=np.zeros(nclass)\n",
    "            fp=np.zeros(nclass)\n",
    "            tn=np.zeros(nclass)\n",
    "            \n",
    "            for p in range(nclass):\n",
    "                tp[p]=cmat[p][p]\n",
    "                for q in range(nclass):\n",
    "                    if q!=p:\n",
    "                        fn[p]+=cmat[p][q]\n",
    "                        fp[p]+=cmat[q][p]\n",
    "                tn[p]=N-tp[p]-fn[p]-fp[p]\n",
    "                \n",
    "                recall[p]=round(float(tp[p])/float(tp[p]+fn[p]+1e-6), 3)\n",
    "                precision[p]=round(float(tp[p])/float(tp[p]+fp[p]+1e-6), 3)\n",
    "                accuracy[p]=round(float(tp[p]+tn[p])/float(N), 3)\n",
    "                bacc[p]=round(0.5*(float(tp[p])/float(tp[p]+fn[p])+float(tn[p])/float(tn[p]+fp[p])), 3)\n",
    "                hss[p]=round(2*float(tp[p]*tn[p]-fp[p]*fn[p])/float((tp[p]+fn[p])*(fn[p]+tn[p])+(tp[p]+fp[p])*(fp[p]+tn[p])), 3)\n",
    "                tss[p]=round(float(tp[p])/float(tp[p]+fn[p]+1e-6)-float(fp[p])/float(fp[p]+tn[p]+1e-6), 3)\n",
    "                \n",
    "            if tss[0] > max_tss[idx]:\n",
    "                max_tss[idx]=tss[0]\n",
    "                max_bacc[idx]=bacc[0]\n",
    "                max_hss[idx]=hss[0]\n",
    "                max_recall0[idx]=recall[0]\n",
    "                max_precision0[idx]=precision[0]\n",
    "                max_recall1[idx]=recall[1]\n",
    "                max_precision1[idx]=precision[1]\n",
    "                max_acc[idx]=accuracy[0]\n",
    "                \n",
    "            recall0list[idx].append(recall[0])\n",
    "            recall1list[idx].append(recall[1])\n",
    "            precision0list[idx].append(precision[0])\n",
    "            precision1list[idx].append(precision[1])\n",
    "            acclist[idx].append(accuracy[0])\n",
    "            bacclist[idx].append(bacc[0])\n",
    "            tsslist[idx].append(tss[0])\n",
    "            hsslist[idx].append(hss[0])\n",
    "            \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_recall0_list = []\n",
    "std_recall0_list = []\n",
    "avg_precision0_list = []\n",
    "std_precision0_list = []\n",
    "avg_acc_list = []\n",
    "std_acc_list = []\n",
    "avg_bacc_list = []\n",
    "std_bacc_list = []\n",
    "avg_hss_list = []\n",
    "std_hss_list = []\n",
    "avg_tss_list = []\n",
    "std_tss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr_list, tpr_list)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th in range(len(thlist)):\n",
    "    idx=th\n",
    "    thresh=thlist[th]\n",
    "    avg_recall0=np.mean(np.array(recall0list[idx]))\n",
    "    std_recall0=np.std(np.array(recall0list[idx]))\n",
    "    \n",
    "    avg_recall1=np.mean(np.array(recall1list[idx]))\n",
    "    std_recall1=np.std(np.array(recall1list[idx]))\n",
    "    \n",
    "    avg_precision0=np.mean(np.array(precision0list[idx]))\n",
    "    std_precision0=np.std(np.array(precision0list[idx]))\n",
    "    \n",
    "    avg_precision1=np.mean(np.array(precision1list[idx]))\n",
    "    std_precision1=np.std(np.array(precision1list[idx]))\n",
    "    \n",
    "    avg_acc=np.mean(np.array(acclist[idx]))\n",
    "    std_acc=np.std(np.array(acclist[idx]))\n",
    "    \n",
    "    avg_bacc=np.mean(np.array(bacclist[idx]))\n",
    "    std_bacc=np.std(np.array(bacclist[idx]))\n",
    "    max_bacc=np.max(np.array(bacclist[idx]))\n",
    "    min_bacc=np.min(np.array(bacclist[idx]))\n",
    "    \n",
    "    avg_hss=np.mean(np.array(hsslist[idx]))\n",
    "    std_hss=np.std(np.array(hsslist[idx]))\n",
    "    max_hss=np.max(np.array(hsslist[idx]))\n",
    "    min_hss=np.min(np.array(hsslist[idx]))\n",
    "    \n",
    "    avg_tss=np.mean(np.array(tsslist[idx]))\n",
    "    std_tss=np.std(np.array(tsslist[idx]))\n",
    "    max_tss=np.max(np.array(tsslist[idx]))\n",
    "    min_tss=np.min(np.array(tsslist[idx]))\n",
    "    \n",
    "    avg_recall0_list.append(round(avg_recall0, 3))\n",
    "    std_recall0_list.append(round(std_recall0, 3))\n",
    "    avg_precision0_list.append(round(avg_precision0, 3))\n",
    "    std_precision0_list.append(round(std_precision0, 3))\n",
    "    avg_acc_list.append(round(avg_acc, 3))\n",
    "    std_acc_list.append(round(std_acc, 3))\n",
    "    avg_bacc_list.append(round(avg_bacc, 3))\n",
    "    std_bacc_list.append(round(std_bacc, 3))\n",
    "    avg_hss_list.append(round(avg_hss, 3))\n",
    "    std_hss_list.append(round(std_hss, 3))\n",
    "    avg_tss_list.append(round(avg_tss, 3))\n",
    "    std_tss_list.append(round(std_tss, 3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_avg_tss=0.0\n",
    "max_avg_hss=0.0\n",
    "max_avg_recall0=0.0\n",
    "max_avg_tss_idx=-1\n",
    "\n",
    "for idx in range(thlistsize):\n",
    "    if avg_tss_list[idx]>max_avg_tss:\n",
    "        max_avg_tss=avg_tss_list[idx]\n",
    "        max_avg_hss=avg_hss_list[idx]\n",
    "        max_avg_recall0=avg_recall0_list[idx]\n",
    "        max_avg_tss_idx=idx\n",
    "    elif avg_tss_list[idx]==max_avg_tss:\n",
    "        if avg_hss_list[idx]>max_avg_hss:\n",
    "            max_avg_tss=avg_tss_list[idx]\n",
    "            max_avg_hss=avg_hss_list[idx]\n",
    "            max_avg_recall0=avg_recall0_list[idx]\n",
    "            max_avg_tss_idx=idx\n",
    "        elif avg_hss_list[idx]==max_avg_hss:\n",
    "            if avg_recall0_list[idx]>max_avg_recall0:\n",
    "                max_avg_tss=avg_tss_list[idx]\n",
    "                max_avg_hss=avg_hss_list[idx]\n",
    "                max_avg_recall0=avg_recall0_list[idx]\n",
    "                max_avg_tss_idx=idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.44\n",
      "\n",
      "avg recall: 0.744 (0.012)\n",
      "\n",
      "avg precision: 0.52 (0.017)\n",
      "\n",
      "avg acc: 0.815 (0.009)\n",
      "\n",
      "avg bacc: 0.788 (0.006)\n",
      "\n",
      "avg hss: 0.496 (0.017)\n",
      "\n",
      "avg tss: 0.577 (0.012)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('threshold: ' +str(thlist[max_avg_tss_idx])+ '\\n')\n",
    "\n",
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 10 iterations (40epochs) WITH Drop WITH activ.reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.6\n",
      "\n",
      "avg recall: 0.728 (0.015)\n",
      "\n",
      "avg precision: 0.564 (0.017)\n",
      "\n",
      "avg acc: 0.837 (0.007)\n",
      "\n",
      "avg bacc: 0.796 (0.003)\n",
      "\n",
      "avg hss: 0.533 (0.011)\n",
      "\n",
      "avg tss: 0.592 (0.006)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('threshold: ' +str(thlist[max_avg_tss_idx])+ '\\n')\n",
    "\n",
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 10 iterations(25 epochs) WITH drop. WITH activity regul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.5\n",
      "\n",
      "avg recall: 0.737 (0.009)\n",
      "\n",
      "avg precision: 0.539 (0.012)\n",
      "\n",
      "avg acc: 0.825 (0.006)\n",
      "\n",
      "avg bacc: 0.792 (0.003)\n",
      "\n",
      "avg hss: 0.512 (0.01)\n",
      "\n",
      "avg tss: 0.584 (0.005)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('threshold: ' +str(thlist[max_avg_tss_idx])+ '\\n')\n",
    "\n",
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Results for 10 iterations(14 epochs) WITH dorop WITH activty regul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.734 (0.021)\n",
      "\n",
      "avg precision: 0.523 (0.022)\n",
      "\n",
      "avg acc: 0.817 (0.011)\n",
      "\n",
      "avg bacc: 0.785 (0.002)\n",
      "\n",
      "avg hss: 0.495 (0.015)\n",
      "\n",
      "avg tss: 0.571 (0.005)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 10 ieration(7 epochs) WITH dropout WITH activity regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.7 (0.016)\n",
      "\n",
      "avg precision: 0.507 (0.012)\n",
      "\n",
      "avg acc: 0.808 (0.007)\n",
      "\n",
      "avg bacc: 0.767 (0.006)\n",
      "\n",
      "avg hss: 0.467 (0.012)\n",
      "\n",
      "avg tss: 0.535 (0.013)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.706 (0.014)\n",
      "\n",
      "avg precision: 0.494 (0.016)\n",
      "\n",
      "avg acc: 0.801 (0.009)\n",
      "\n",
      "avg bacc: 0.765 (0.005)\n",
      "\n",
      "avg hss: 0.455 (0.014)\n",
      "\n",
      "avg tss: 0.53 (0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 10 iterations(7 epochs ) WITH dopout WHIOUT activity regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.706 (0.014)\n",
      "\n",
      "avg precision: 0.494 (0.016)\n",
      "\n",
      "avg acc: 0.801 (0.009)\n",
      "\n",
      "avg bacc: 0.765 (0.005)\n",
      "\n",
      "avg hss: 0.455 (0.014)\n",
      "\n",
      "avg tss: 0.53 (0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Results for 10 iterations (7 epochs) without activity regularizations without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.713 (0.019)\n",
      "\n",
      "avg precision: 0.493 (0.015)\n",
      "\n",
      "avg acc: 0.8 (0.008)\n",
      "\n",
      "avg bacc: 0.767 (0.005)\n",
      "\n",
      "avg hss: 0.457 (0.011)\n",
      "\n",
      "avg tss: 0.535 (0.009)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " results for 100 iterations (7 epochs) without activity regular, without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"./output.txt\", \"w\")\n",
    "f.write('results for 100 iterations (7 epochs) without activity regular, without dropout\\n')\n",
    "f.write('\\n')\n",
    "f.write('thresh = '+str(thlist[max_avg_tss_idx])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.709 (0.017)\n",
      "\n",
      "avg precision: 0.501 (0.021)\n",
      "\n",
      "avg acc: 0.804 (0.011)\n",
      "\n",
      "avg bacc: 0.768 (0.006)\n",
      "\n",
      "avg hss: 0.464 (0.018)\n",
      "\n",
      "avg tss: 0.537 (0.011)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('avg recall: '+str(avg_recall0_list[max_avg_tss_idx]) +\n",
    "      ' (' + str(std_recall0_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg precision: ' + str(avg_precision0_list[max_avg_tss_idx])\n",
    "      +' (' + str(std_precision0_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg acc: ' + str(avg_acc_list[max_avg_tss_idx]) \n",
    "      + ' (' + str(std_acc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg bacc: ' +str(avg_bacc_list[max_avg_tss_idx]) + ' ('\n",
    "      + str(std_bacc_list[max_avg_tss_idx]) + ')\\n')\n",
    "print('avg hss: ' + str(avg_hss_list[max_avg_tss_idx]) \n",
    "      + ' (' +str(std_hss_list[max_avg_tss_idx]) +')\\n')\n",
    "print('avg tss: ' + str(avg_tss_list[max_avg_tss_idx])+' ('\n",
    "      +str(std_tss_list[max_avg_tss_idx])+ ')\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./C/normalized_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>flare</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>NOAA</th>\n",
       "      <th>HARP</th>\n",
       "      <th>TOTUSJH</th>\n",
       "      <th>Cdec</th>\n",
       "      <th>TOTUSJZ</th>\n",
       "      <th>Chis1d</th>\n",
       "      <th>USFLUX</th>\n",
       "      <th>...</th>\n",
       "      <th>Bhis1d</th>\n",
       "      <th>Bhis</th>\n",
       "      <th>Mhis1d</th>\n",
       "      <th>EPSZ</th>\n",
       "      <th>MEANGBH</th>\n",
       "      <th>MEANJZH</th>\n",
       "      <th>MEANALP</th>\n",
       "      <th>Xhis</th>\n",
       "      <th>Xdec</th>\n",
       "      <th>Xhis1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>N</td>\n",
       "      <td>2010-05-03T05:34:22.60Z</td>\n",
       "      <td>11063</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.567834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.712481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535443</td>\n",
       "      <td>0.716336</td>\n",
       "      <td>1.364450</td>\n",
       "      <td>-0.623816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>N</td>\n",
       "      <td>2010-05-03T06:34:22.60Z</td>\n",
       "      <td>11063</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.571116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.195688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.714876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.168495</td>\n",
       "      <td>1.065008</td>\n",
       "      <td>0.786958</td>\n",
       "      <td>-0.619474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>N</td>\n",
       "      <td>2010-05-03T07:34:22.60Z</td>\n",
       "      <td>11063</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.559336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.714202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.301990</td>\n",
       "      <td>1.212839</td>\n",
       "      <td>0.362941</td>\n",
       "      <td>-0.617128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>N</td>\n",
       "      <td>2010-05-03T08:34:22.60Z</td>\n",
       "      <td>11063</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.584086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.256048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.713703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296428</td>\n",
       "      <td>1.261323</td>\n",
       "      <td>0.067030</td>\n",
       "      <td>-0.610767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>N</td>\n",
       "      <td>2010-05-03T09:34:22.60Z</td>\n",
       "      <td>11063</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.574319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.275039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.710402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164787</td>\n",
       "      <td>1.369495</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>-0.609721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127704</th>\n",
       "      <td>Positive</td>\n",
       "      <td>C2.5</td>\n",
       "      <td>2014-01-02T15:46:09.10Z</td>\n",
       "      <td>11936</td>\n",
       "      <td>3535</td>\n",
       "      <td>3.006697</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.422409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.307591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-1.110994</td>\n",
       "      <td>-0.308918</td>\n",
       "      <td>0.133939</td>\n",
       "      <td>2.849963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127705</th>\n",
       "      <td>Positive</td>\n",
       "      <td>C2.5</td>\n",
       "      <td>2014-01-02T16:46:09.10Z</td>\n",
       "      <td>11936</td>\n",
       "      <td>3535</td>\n",
       "      <td>2.975592</td>\n",
       "      <td>0.004770</td>\n",
       "      <td>0.804194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.339006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-1.148076</td>\n",
       "      <td>-0.376186</td>\n",
       "      <td>0.162697</td>\n",
       "      <td>2.892229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127706</th>\n",
       "      <td>Positive</td>\n",
       "      <td>C2.5</td>\n",
       "      <td>2014-01-02T17:46:09.10Z</td>\n",
       "      <td>11936</td>\n",
       "      <td>3535</td>\n",
       "      <td>3.113931</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.653550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.331007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-1.268592</td>\n",
       "      <td>-0.427330</td>\n",
       "      <td>0.074292</td>\n",
       "      <td>2.902795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127707</th>\n",
       "      <td>Positive</td>\n",
       "      <td>C2.5</td>\n",
       "      <td>2014-01-02T18:46:09.10Z</td>\n",
       "      <td>11936</td>\n",
       "      <td>3535</td>\n",
       "      <td>3.000728</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.318120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.369386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.288987</td>\n",
       "      <td>-0.529957</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>2.857465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127708</th>\n",
       "      <td>Negative</td>\n",
       "      <td>N</td>\n",
       "      <td>2014-01-05T08:46:09.30Z</td>\n",
       "      <td>11936</td>\n",
       "      <td>3535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127709 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label flare                timestamp   NOAA  HARP   TOTUSJH  \\\n",
       "0       Negative     N  2010-05-03T05:34:22.60Z  11063    11 -0.567834   \n",
       "1       Negative     N  2010-05-03T06:34:22.60Z  11063    11 -0.571116   \n",
       "2       Negative     N  2010-05-03T07:34:22.60Z  11063    11 -0.559336   \n",
       "3       Negative     N  2010-05-03T08:34:22.60Z  11063    11 -0.584086   \n",
       "4       Negative     N  2010-05-03T09:34:22.60Z  11063    11 -0.574319   \n",
       "...          ...   ...                      ...    ...   ...       ...   \n",
       "127704  Positive  C2.5  2014-01-02T15:46:09.10Z  11936  3535  3.006697   \n",
       "127705  Positive  C2.5  2014-01-02T16:46:09.10Z  11936  3535  2.975592   \n",
       "127706  Positive  C2.5  2014-01-02T17:46:09.10Z  11936  3535  3.113931   \n",
       "127707  Positive  C2.5  2014-01-02T18:46:09.10Z  11936  3535  3.000728   \n",
       "127708  Negative     N  2014-01-05T08:46:09.30Z  11936  3535  0.000000   \n",
       "\n",
       "            Cdec   TOTUSJZ  Chis1d    USFLUX  ...  Bhis1d      Bhis    Mhis1d  \\\n",
       "0       0.000000 -0.125322     0.0 -0.712481  ...     0.0  0.000000  0.000000   \n",
       "1       0.000000 -0.195688     0.0 -0.714876  ...     0.0  0.000000  0.000000   \n",
       "2       0.000000 -0.160199     0.0 -0.714202  ...     0.0  0.000000  0.000000   \n",
       "3       0.000000 -0.256048     0.0 -0.713703  ...     0.0  0.000000  0.000000   \n",
       "4       0.000000 -0.275039     0.0 -0.710402  ...     0.0  0.000000  0.000000   \n",
       "...          ...       ...     ...       ...  ...     ...       ...       ...   \n",
       "127704  0.005185  0.422409     0.0  2.307591  ...     0.0  0.037736  0.090909   \n",
       "127705  0.004770  0.804194     0.0  2.339006  ...     0.0  0.037736  0.090909   \n",
       "127706  0.004389  0.653550     0.0  2.331007  ...     0.0  0.037736  0.090909   \n",
       "127707  0.004038  0.318120     0.0  2.369386  ...     0.0  0.037736  0.000000   \n",
       "127708  0.000929  0.000000     0.0  0.000000  ...     0.0  0.037736  0.090909   \n",
       "\n",
       "            EPSZ   MEANGBH   MEANJZH   MEANALP  Xhis  Xdec  Xhis1d  \n",
       "0       0.535443  0.716336  1.364450 -0.623816   0.0   0.0     0.0  \n",
       "1      -0.168495  1.065008  0.786958 -0.619474   0.0   0.0     0.0  \n",
       "2      -0.301990  1.212839  0.362941 -0.617128   0.0   0.0     0.0  \n",
       "3      -0.296428  1.261323  0.067030 -0.610767   0.0   0.0     0.0  \n",
       "4      -0.164787  1.369495 -0.025539 -0.609721   0.0   0.0     0.0  \n",
       "...          ...       ...       ...       ...   ...   ...     ...  \n",
       "127704 -1.110994 -0.308918  0.133939  2.849963   0.0   0.0     0.0  \n",
       "127705 -1.148076 -0.376186  0.162697  2.892229   0.0   0.0     0.0  \n",
       "127706 -1.268592 -0.427330  0.074292  2.902795   0.0   0.0     0.0  \n",
       "127707 -1.288987 -0.529957  0.072162  2.857465   0.0   0.0     0.0  \n",
       "127708  0.000000  0.000000  0.000000  0.000000   0.0   0.0     0.0  \n",
       "\n",
       "[127709 rows x 45 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
